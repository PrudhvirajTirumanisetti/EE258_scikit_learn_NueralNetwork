{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Text Mining (30 points)\n",
    "Tired of [Rotten Tomatoes](http://www.rottentomatoes.com/)? This is your chance to make your own movie review aggregator, slightly-smelly-tomatoes. In this assignment, you will be revisiting some of the NLTK commands from the Text Mining lecture and applying them to a text classification task. First, you'll classify movie reviews in a slightly-more-complicated version of the problem we saw in class.  Next, you'll produce document similarity scores using the Reuters Corpus of 10,000 news articles. This will involve loading text data, pre-processing the text (tokenization, stemming, punctuation and stopword removal), performing simple feature generation for text data, and using the generated features to compute document similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preliminaries\n",
    "\n",
    "#Show plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets, preprocessing, cross_validation, feature_extraction\n",
    "from sklearn import linear_model, svm, metrics, ensemble, neighbors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib2\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import svm\n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification\n",
    "Here's a cleaner version of the Movie Review Sentiment Classification example from class. Make sure you understand what each of these components does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583820\n",
      "After stop words removal\n",
      "964269\n",
      "After removing punctuation\n",
      "717292\n",
      "Completed preprocessing\n",
      "accuracy    0.7865\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Movie Review Sentiment Classification Task\n",
    "\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "def load_movie_review_data():\n",
    "    # Generate lists of positive and negative reviews\n",
    "    negids = movie_reviews.fileids('neg')\n",
    "    posids = movie_reviews.fileids('pos')\n",
    "    return [negids, posids]\n",
    "\n",
    "\n",
    "def compute_preprocessing_features():\n",
    "    # Compute word frequencies in corpus and select the top 2500 words\n",
    "    all_words_list = movie_reviews.words()\n",
    "    \n",
    "    # ------ REMOVE STOP WORDS \n",
    "    print len(all_words_list)\n",
    "    stop = stopwords.words('english')\n",
    "    all_words_list = [word for word in all_words_list if word not in stop]\n",
    "    print \"After stop words removal\"\n",
    "    print len(all_words_list)\n",
    "\n",
    "    # ------ REMOVE PUNCTUATION\n",
    "    all_words_list = [''.join(c for c in s if c not in string.punctuation) for s in all_words_list]\n",
    "    all_words_list = filter(None, all_words_list)\n",
    "    print \"After removing punctuation\"\n",
    "    print len(all_words_list)\n",
    "\n",
    "#     # ------ LANCASTER STEMMER\n",
    "#     lancaster = nltk.LancasterStemmer()\n",
    "#     all_words_list = [lancaster.stem(t) for t in all_words_list]\n",
    "\n",
    "    # ------ PORTER STEMMER \n",
    "    porter = nltk.PorterStemmer()\n",
    "    all_words_list = [porter.stem(t) for t in all_words_list]\n",
    "\n",
    "    all_words = nltk.FreqDist(w.lower() for w in all_words_list)\n",
    "    word_features = [fpair[0] for fpair in list(all_words.most_common(2500))]\n",
    "    return word_features\n",
    "\n",
    "def generate_review_features(negids, posids):\n",
    "    # Generate features for positive and negative reviews\n",
    "    negfeats = [(document_features(movie_reviews.words(fileids=[f]), word_features), 'neg') for f in negids]\n",
    "    posfeats = [(document_features(movie_reviews.words(fileids=[f]), word_features), 'pos') for f in posids]\n",
    "    return [negfeats, posfeats]\n",
    "\n",
    "    \n",
    "def generate_train_test_split(negfeats, posfeats, train, test):\n",
    "    # Generate a train-test split\n",
    "    combined_feats = negfeats + posfeats;\n",
    "    trainfeats = [ combined_feats[i] for i in train ]\n",
    "    testfeats = [ combined_feats[i] for i in test ]\n",
    "    #print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    "    return [trainfeats, testfeats]\n",
    "\n",
    "def train_classifier(trainfeats):\n",
    "    # Train a classifier\n",
    "    classifier = nltk.NaiveBayesClassifier.train(trainfeats)\n",
    "    return classifier\n",
    "\n",
    "def evaluate_performance(classifier, testfeats):\n",
    "    # Evaluate classifier\n",
    "    return nltk.classify.util.accuracy(classifier, testfeats)\n",
    "    #classifier.show_most_informative_features()\n",
    "\n",
    "[neg_fileids, pos_fileids] = load_movie_review_data()\n",
    "word_features = compute_preprocessing_features()\n",
    "[neg_features, pos_features] = generate_review_features(neg_fileids, pos_fileids)\n",
    "\n",
    "print \"Completed preprocessing\"\n",
    "\n",
    "foldnum=0\n",
    "review_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(neg_features)+len(pos_features), random_state=20160217, \n",
    "                                          shuffle=True, n_folds=5):\n",
    "    foldnum+=1\n",
    "    [review_train, review_test] = generate_train_test_split(neg_features, pos_features, train, test)\n",
    "    clfr = train_classifier(review_train)\n",
    "    review_results.loc[foldnum,'accuracy'] = evaluate_performance(clfr, review_test)\n",
    "\n",
    "print review_results.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Movie Rating Sentiment Classification (15 points):\n",
    "The movie review example above has a number of issues. The words are not stemmed, stopwords haven't been removed, punctuation is still included, and the classifier only uses absence and presence information for each word. Perform each of these steps below to improve the classifier, and report the change in classifier performance. \n",
    "1. Remove stopwords\n",
    "2. Remove punctuation\n",
    "3. Use the Lancaster stemmer to stem the words\n",
    "4. Instead, use the Porter stemmer to stem the words\n",
    "5. Using an SVM classifier with word counts instead of boolean flags (using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit-learn)\n",
    "6. Using the SVM again, but instead of using counts, perform the TF-IDF transformation to get the features (using [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) from scikit-learn)\n",
    "\n",
    "(Hint: I suggest using functions similar to the ones above, and specifying some boolean flags that control each step. You may need to implement new methods for train_classifier and evaluate_performance, and possibly return different features from compute_preprocessing_features and document_features based on the flags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Answers 1 </h1>\n",
    "<p> 1. Remove stopwords Accuracy: 80.85%</p>\n",
    "<p> 2. Remove punctuation Accuracy: 80.08%</p>\n",
    "<p> 3. Lancaster Accuracy: 75.85% </p>\n",
    "<p> 4. Porter stopwords Accuracy: 78.65%</p>\n",
    "<p> 5. SVM with CountVectorizer: 66.6%</p>\n",
    "<p> 6. SVM with TF-IDF: 66.6%</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583820\n",
      "After stop words removal\n",
      "964269\n",
      "After removing punctuation\n",
      "717292\n",
      "Completed preprocessing\n",
      "accuracy    0.666\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import svm\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "# ------- BOOLEAN FEATURES\n",
    "# def document_features(document, word_features):\n",
    "#     document_words = set(document)\n",
    "#     features = {}\n",
    "#     for word in word_features:\n",
    "#         features['contains({})'.format(word)] = (word in document_words)\n",
    "#     return features\n",
    "\n",
    "\n",
    "# # -------- COUNT VECTORIZER\n",
    "# def document_features(document, word_features):\n",
    "#     count_vect = CountVectorizer()\n",
    "#     document_words = count_vect.fit_transform(document)\n",
    "#     features = {}\n",
    "#     for word in word_features:\n",
    "#         count = count_vect.vocabulary_.get(word)\n",
    "#         if count == None:\n",
    "#             count = 0\n",
    "#         features['contains({})'.format(word)] = count\n",
    "#     return features\n",
    "\n",
    "# -------TFIDF VECTORIZER\n",
    "def document_features(document, word_features):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    document_words = tfidf_vectorizer.fit_transform(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        count = tfidf_vectorizer.vocabulary_.get(word)\n",
    "        if count == None:\n",
    "            count = 0\n",
    "        features['contains({})'.format(word)] = count\n",
    "    return features\n",
    "\n",
    "def load_movie_review_data():\n",
    "    # Generate lists of positive and negative reviews\n",
    "    negids = movie_reviews.fileids('neg')\n",
    "    posids = movie_reviews.fileids('pos')\n",
    "    return [negids, posids]\n",
    "\n",
    "def compute_preprocessing_features():\n",
    "    # Compute word frequencies in corpus and select the top 2500 words\n",
    "    all_words_list = movie_reviews.words()\n",
    "    \n",
    "    # ------ REMOVE STOP WORDS \n",
    "    print len(all_words_list)\n",
    "    stop = stopwords.words('english')\n",
    "    all_words_list = [word for word in all_words_list if word not in stop]\n",
    "    print \"After stop words removal\"\n",
    "    print len(all_words_list)\n",
    "\n",
    "    # ------ REMOVE PUNCTUATION\n",
    "    all_words_list = [''.join(c for c in s if c not in string.punctuation) for s in all_words_list]\n",
    "    all_words_list = filter(None, all_words_list)\n",
    "    print \"After removing punctuation\"\n",
    "    print len(all_words_list)\n",
    "\n",
    "#     # ------ LANCASTER STEMMER\n",
    "#     lancaster = nltk.LancasterStemmer()\n",
    "#     all_words_list = [lancaster.stem(t) for t in all_words_list]\n",
    "\n",
    "    # ------ PORTER STEMMER \n",
    "    porter = nltk.PorterStemmer()\n",
    "    all_words_list = [porter.stem(t) for t in all_words_list]\n",
    "\n",
    "    all_words = nltk.FreqDist(w.lower() for w in all_words_list)\n",
    "    word_features = [fpair[0] for fpair in list(all_words.most_common(2500))]\n",
    "    return word_features\n",
    "\n",
    "def generate_review_features(negids, posids):\n",
    "    # Generate features for positive and negative reviews\n",
    "    negfeats = [(document_features(movie_reviews.words(fileids=[f]), word_features), 'neg') for f in negids]\n",
    "    posfeats = [(document_features(movie_reviews.words(fileids=[f]), word_features), 'pos') for f in posids]\n",
    "    return [negfeats, posfeats]\n",
    "\n",
    "    \n",
    "def generate_train_test_split(negfeats, posfeats, train, test):\n",
    "    # Generate a train-test split\n",
    "    combined_feats = negfeats + posfeats;\n",
    "    trainfeats = [ combined_feats[i] for i in train ]\n",
    "    testfeats = [ combined_feats[i] for i in test ]\n",
    "    #print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    "    return [trainfeats, testfeats]\n",
    "\n",
    "def train_classifier(trainfeats):\n",
    "    # Train a classifier\n",
    "    classif = SklearnClassifier(LinearSVC())\n",
    "    classifier = classif.train(trainfeats)\n",
    "    return classifier\n",
    "\n",
    "def evaluate_performance(classifier, testfeats):\n",
    "    # Evaluate classifier\n",
    "    return nltk.classify.util.accuracy(classifier, testfeats)\n",
    "    #classifier.show_most_informative_features()\n",
    "\n",
    "[neg_fileids, pos_fileids] = load_movie_review_data()\n",
    "word_features = compute_preprocessing_features()\n",
    "\n",
    "[neg_features, pos_features] = generate_review_features(neg_fileids, pos_fileids)\n",
    "\n",
    "print \"Completed preprocessing\"\n",
    "\n",
    "foldnum=0\n",
    "review_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(neg_features)+len(pos_features), random_state=20160217, \n",
    "                                          shuffle=True, n_folds=5):\n",
    "    foldnum+=1\n",
    "    [review_train, review_test] = generate_train_test_split(neg_features, pos_features, train, test)\n",
    "    clfr = train_classifier(review_train)\n",
    "    review_results.loc[foldnum,'accuracy'] = evaluate_performance(clfr, review_test)\n",
    "\n",
    "print review_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity\n",
    "In this section, you'll build the basic components for document retrieval and relevancy by computing [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). This measure is actually fairly straightforward to compute: it's simply the dot product of the two document vectors normalized by the document length. Commonly, the document vectors aren't simply word counts, but TF-IDF features for the documents to put greater emphasis on salient words.\n",
    "\n",
    "For example the cosine similarity, after TF-IDF normalization, of:\n",
    "   * **`a little bird`** and **`a little bird`** is 1\n",
    "   * **`a little bird`** and **`a little bird chirps`** is 0.71\n",
    "   * **`a little bird chirps`** and **`a big dog barks`** is 0 (think about why this is the case, even though they have \"a\" in common)\n",
    "\n",
    "\n",
    "You'll be using this same principle to find similar news articles in a large corpus of news data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Finding Similar Documents (15 points)\n",
    "1. Create a Text Collection from the Reuters Corpus (hint: use the function `TextCollection()`)\n",
    "2. Tokenize each document in the Reuters Corpus into words\n",
    "3. Remove punctuation\n",
    "4. Remove stop words\n",
    "5. Stem the words using PorterStemmer\n",
    "6. Compute  TF-IDF features for all of the pre-processed documents in the Reuters corpus.\n",
    "7. Write a function that computes the cosine-similarity between two documents using TF-IDF features. The function should be named cosine_sim and should take two text documents as input -- e.g., cosine_sim(text1, text2).\n",
    "8. Create a training set of fileids that contain 'train' (similar to what we did for 'pos' and 'neg' in the previous question)\n",
    "9. Find and report the most similar documents in the training set for the following fileids:\n",
    "    * test/14826\n",
    "    * test/14998\n",
    "    * test/15110\n",
    "    * test/15197\n",
    "    * test/15348"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Answers 2 </h1>\n",
    "<p> 1. See bode below for the preprocessing stage </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1720901\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "document_ids = reuters.fileids()\n",
    "doc_words = nltk.TextCollection(reuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ REMOVE PUNCTUATION\n",
    "doc_words = [''.join(c for c in s if c not in string.punctuation) for s in doc_words]\n",
    "doc_words = filter(None, doc_words)\n",
    "\n",
    "# ------ REMOVE STOP WORDS \n",
    "stop = stopwords.words('english')\n",
    "doc_words = [word for word in doc_words if word not in stop]\n",
    "\n",
    "# ------ PORTER STEMMER \n",
    "porter = nltk.PorterStemmer()\n",
    "doc_words = [porter.stem(t) for t in doc_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "def document_features(document, word_features):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    document_words = tfidf_vectorizer.fit_transform(document)\n",
    "    features = {}\n",
    "\n",
    "    for word in word_features:\n",
    "        count = tfidf_vectorizer.vocabulary_.get(word)\n",
    "        if count == None:\n",
    "            count = 0\n",
    "        features['contains({})'.format(word)] = count\n",
    "\n",
    "    return features\n",
    "\n",
    "doc_features = []\n",
    "for fileid in document_ids:\n",
    "    doc_features.append(document_features(reuters.words(fileid), doc_words))\n",
    "\n",
    "print len(doc_words)\n",
    "print len(doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
