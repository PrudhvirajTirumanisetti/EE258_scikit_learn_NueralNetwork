{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prudhviraj Tirumanisetti\n",
    "EE 258 ID:011489881"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Text Mining (30 points)\n",
    "Tired of [Rotten Tomatoes](http://www.rottentomatoes.com/)? This is your chance to make your own movie review aggregator, slightly-smelly-tomatoes. In this assignment, you will be revisiting some of the NLTK commands from the Text Mining lecture and applying them to a text classification task. First, you'll classify movie reviews in a slightly-more-complicated version of the problem we saw in class.  Next, you'll produce document similarity scores using the Reuters Corpus of 10,000 news articles. This will involve loading text data, pre-processing the text (tokenization, stemming, punctuation and stopword removal), performing simple feature generation for text data, and using the generated features to compute document similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preliminaries\n",
    "\n",
    "#Show plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets, preprocessing, cross_validation, feature_extraction\n",
    "from sklearn import linear_model, svm, metrics, ensemble, neighbors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib2\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification\n",
    "Here's a cleaner version of the Movie Review Sentiment Classification example from class. Make sure you understand what each of these components does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed preprocessing\n",
      "accuracy    0.805\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Movie Review Sentiment Classification Task\n",
    "\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "def load_movie_review_data():\n",
    "    # Generate lists of positive and negative reviews\n",
    "    negids = movie_reviews.fileids('neg')\n",
    "    posids = movie_reviews.fileids('pos')\n",
    "    return [negids, posids]\n",
    "\n",
    "\n",
    "def compute_preprocessing_features():\n",
    "    # Compute word frequencies in corpus and select the top 2500 words\n",
    "    all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "    word_features = [fpair[0] for fpair in list(all_words.most_common(2500))]\n",
    "    return word_features\n",
    "\n",
    "def generate_review_features(negids, posids):\n",
    "    # Generate features for positive and negative reviews\n",
    "    negfeats = [(document_features(movie_reviews.words(fileids=[f]), word_features), 'neg') for f in negids]\n",
    "    posfeats = [(document_features(movie_reviews.words(fileids=[f]), word_features), 'pos') for f in posids]\n",
    "    return [negfeats, posfeats]\n",
    "\n",
    "    \n",
    "def generate_train_test_split(negfeats, posfeats, train, test):\n",
    "    # Generate a train-test split\n",
    "    combined_feats = negfeats + posfeats;\n",
    "    trainfeats = [ combined_feats[i] for i in train ]\n",
    "    testfeats = [ combined_feats[i] for i in test ]\n",
    "    #print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    "    return [trainfeats, testfeats]\n",
    "\n",
    "def train_classifier(trainfeats):\n",
    "    # Train a classifier\n",
    "    classifier = nltk.NaiveBayesClassifier.train(trainfeats)\n",
    "    return classifier\n",
    "\n",
    "def evaluate_performance(classifier, testfeats):\n",
    "    # Evaluate classifier\n",
    "    return nltk.classify.util.accuracy(classifier, testfeats)\n",
    "    #classifier.show_most_informative_features()\n",
    "\n",
    "[neg_fileids, pos_fileids] = load_movie_review_data()\n",
    "word_features = compute_preprocessing_features()\n",
    "[neg_features, pos_features] = generate_review_features(neg_fileids, pos_fileids)\n",
    "\n",
    "print \"Completed preprocessing\"\n",
    "\n",
    "foldnum=0\n",
    "review_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(neg_features)+len(pos_features), random_state=20160217, \n",
    "                                          shuffle=True, n_folds=5):\n",
    "    foldnum+=1\n",
    "    [review_train, review_test] = generate_train_test_split(neg_features, pos_features, train, test)\n",
    "    clfr = train_classifier(review_train)\n",
    "    review_results.loc[foldnum,'accuracy'] = evaluate_performance(clfr, review_test)\n",
    "\n",
    "print review_results.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "2379\n",
      "Completed preprocessing\n",
      "accuracy    0.8055\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# A) Stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "[neg_fileids, pos_fileids] = load_movie_review_data()\n",
    "word_features = compute_preprocessing_features()\n",
    "\n",
    "print len(word_features)\n",
    "word_features = [word for word in word_features if word not in stop]\n",
    "print len(word_features)\n",
    "\n",
    "[neg_features, pos_features] = generate_review_features(neg_fileids, pos_fileids)\n",
    "\n",
    "print \"Completed preprocessing\"\n",
    "\n",
    "foldnum=0\n",
    "review_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(neg_features)+len(pos_features), random_state=20160217, \n",
    "                                          shuffle=True, n_folds=5):\n",
    "    foldnum+=1\n",
    "    [review_train, review_test] = generate_train_test_split(neg_features, pos_features, train, test)\n",
    "    clfr = train_classifier(review_train)\n",
    "    review_results.loc[foldnum,'accuracy'] = evaluate_performance(clfr, review_test)\n",
    "\n",
    "print review_results.mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RegexpTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a460b31f1633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RegexpTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# B) Remove punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "[neg_fileids, pos_fileids] = load_movie_review_data()\n",
    "word_features = compute_preprocessing_features()\n",
    "\n",
    "# Remove stop words\n",
    "print len(word_features)\n",
    "word_features = [word for word in word_features if word not in stop]\n",
    "print len(word_features)\n",
    "\n",
    "# Remove punctuation\n",
    "word_features = [''.join(c for c in s if c not in string.punctuation) for s in word_features]\n",
    "word_features = filter(None, word_features)\n",
    "\n",
    "print len(word_features)\n",
    "\n",
    "[neg_features, pos_features] = generate_review_features(neg_fileids, pos_fileids)\n",
    "\n",
    "print \"Completed preprocessing\"\n",
    "\n",
    "foldnum=0\n",
    "review_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(neg_features)+len(pos_features), random_state=20160217, \n",
    "                                          shuffle=True, n_folds=5):\n",
    "    foldnum+=1\n",
    "    [review_train, review_test] = generate_train_test_split(neg_features, pos_features, train, test)\n",
    "    clfr = train_classifier(review_train)\n",
    "    review_results.loc[foldnum,'accuracy'] = evaluate_performance(clfr, review_test)\n",
    "\n",
    "print review_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print movie_reviews\n",
    "# import string\n",
    "# negids = movie_reviews.fileids('neg')\n",
    "# # for i in range(0, len(word_features)):\n",
    "# #     print word_features[i]\n",
    "\n",
    "# print len(word_features)\n",
    "# # [s.translate(None, string.punctuation) for s in movie_reviews]\n",
    "# word_features = [''.join(c for c in s if c not in string.punctuation) for s in word_features]\n",
    "# word_features = filter(None, word_features)\n",
    "# # for i in range(0, len(word_features)):\n",
    "# #     print word_features[i]\n",
    "# print len(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# C) Lancaster Stemmer \n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "[neg_fileids, pos_fileids] = load_movie_review_data()\n",
    "word_features = compute_preprocessing_features()\n",
    "\n",
    "# Remove stop words\n",
    "print len(word_features)\n",
    "word_features = [word for word in word_features if word not in stop]\n",
    "print \"After stop words removal\"\n",
    "print len(word_features)\n",
    "\n",
    "# Remove punctuation\n",
    "word_features = [''.join(c for c in s if c not in string.punctuation) for s in word_features]\n",
    "word_features = filter(None, word_features)\n",
    "\n",
    "print \"After removing punctuation\"\n",
    "print len(word_features)\n",
    "\n",
    "# Applying Lancaster stemmer\n",
    "word_features = [lancaster.stem(t) for t in word_features]\n",
    "\n",
    "print \"After Lancaster Stemmer\"\n",
    "print len(word_features)\n",
    "\n",
    "[neg_features, pos_features] = generate_review_features(neg_fileids, pos_fileids)\n",
    "\n",
    "print \"Completed preprocessing\"\n",
    "\n",
    "foldnum=0\n",
    "review_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(neg_features)+len(pos_features), random_state=20160217, \n",
    "                                          shuffle=True, n_folds=5):\n",
    "    foldnum+=1\n",
    "    [review_train, review_test] = generate_train_test_split(neg_features, pos_features, train, test)\n",
    "    clfr = train_classifier(review_train)\n",
    "    review_results.loc[foldnum,'accuracy'] = evaluate_performance(clfr, review_test)\n",
    "\n",
    "print review_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D) Porter Stemmer \n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "[neg_fileids, pos_fileids] = load_movie_review_data()\n",
    "word_features = compute_preprocessing_features()\n",
    "\n",
    "# Remove stop words\n",
    "print len(word_features)\n",
    "word_features = [word for word in word_features if word not in stop]\n",
    "print \"After stop words removal\"\n",
    "print len(word_features)\n",
    "\n",
    "# Remove punctuation\n",
    "word_features = [''.join(c for c in s if c not in string.punctuation) for s in word_features]\n",
    "word_features = filter(None, word_features)\n",
    "\n",
    "print \"After removing punctuation\"\n",
    "print len(word_features)\n",
    "\n",
    "# Applying Porter stemmer\n",
    "word_features = [porter.stem(t) for t in word_features]\n",
    "    \n",
    "print \"After Lancaster Stemmer\"\n",
    "print len(word_features)\n",
    "\n",
    "[neg_features, pos_features] = generate_review_features(neg_fileids, pos_fileids)\n",
    "\n",
    "print \"Completed preprocessing\"\n",
    "\n",
    "foldnum=0\n",
    "review_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(neg_features)+len(pos_features), random_state=20160217, \n",
    "                                          shuffle=True, n_folds=5):\n",
    "    foldnum+=1\n",
    "    [review_train, review_test] = generate_train_test_split(neg_features, pos_features, train, test)\n",
    "    clfr = train_classifier(review_train)\n",
    "    review_results.loc[foldnum,'accuracy'] = evaluate_performance(clfr, review_test)\n",
    "\n",
    "print review_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import stem\n",
    "# porter = stem.porter.PorterStemmer()\n",
    "# lancaster = stem.lancaster.LancasterStemmer()\n",
    "\n",
    "# for i in range(0,10):\n",
    "#     print word_features[i]\n",
    "\n",
    "# word_features = [lancaster.stem(t) for t in word_features]\n",
    "# print \"Stemmed--------\"\n",
    "\n",
    "# for i in range(0,10):\n",
    "#     print word_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Movie Review Sentiment Classification Task\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "svm_classifier = SklearnClassifier(LinearSVC())\n",
    "\n",
    "def document_features(document, word_features):\n",
    "    vectorizer = CountVectorizer()\n",
    "    document_words = vectorizer.fit_transform(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (vectorizer.vocabulary_.get(word))\n",
    "    return features\n",
    "\n",
    "def load_movie_review_data():\n",
    "    # Generate lists of positive and negative reviews\n",
    "    negids = movie_reviews.fileids('neg')\n",
    "    posids = movie_reviews.fileids('pos')\n",
    "    return [negids, posids]\n",
    "\n",
    "\n",
    "def compute_preprocessing_features():\n",
    "    # Compute word frequencies in corpus and select the top 2500 words\n",
    "    all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "    word_features = [fpair[0] for fpair in list(all_words.most_common(2500))]\n",
    "    return word_features\n",
    "\n",
    "def generate_review_features(negids, posids):\n",
    "    # Generate features for positive and negative reviews\n",
    "    negfeats = [(document_features(movie_reviews.words(fileids=[f]), word_features), 'neg') for f in negids]\n",
    "    posfeats = [(document_features(movie_reviews.words(fileids=[f]), word_features), 'pos') for f in posids]\n",
    "    return [negfeats, posfeats]\n",
    "\n",
    "    \n",
    "def generate_train_test_split(negfeats, posfeats, train, test):\n",
    "    # Generate a train-test split\n",
    "    combined_feats = negfeats + posfeats;\n",
    "    trainfeats = [ combined_feats[i] for i in train ]\n",
    "    testfeats = [ combined_feats[i] for i in test ]\n",
    "    #print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))\n",
    "    return [trainfeats, testfeats]\n",
    "\n",
    "def train_classifier(trainfeats):\n",
    "    # Train a classifier\n",
    "#     print len(trainfeats)\n",
    "    classifier = svm_classifier.train(trainfeats)\n",
    "    return classifier\n",
    "\n",
    "def evaluate_performance(classifier, testfeats):\n",
    "    # Evaluate classifier\n",
    "    return nltk.classify.util.accuracy(classifier, testfeats)\n",
    "    #classifier.show_most_informative_features()\n",
    "\n",
    "[neg_fileids, pos_fileids] = load_movie_review_data()\n",
    "word_features = compute_preprocessing_features()\n",
    "[neg_features, pos_features] = generate_review_features(neg_fileids, pos_fileids)\n",
    "\n",
    "print \"Completed preprocessing\"\n",
    "\n",
    "foldnum=0\n",
    "review_results = pd.DataFrame()\n",
    "for train, test in cross_validation.KFold(len(neg_features)+len(pos_features), random_state=20160217, \n",
    "                                          shuffle=True, n_folds=5):\n",
    "    foldnum+=1\n",
    "    [review_train, review_test] = generate_train_test_split(neg_features, pos_features, train, test)\n",
    "    clfr = train_classifier(review_train)\n",
    "    review_results.loc[foldnum,'accuracy'] = evaluate_performance(clfr, review_test)\n",
    "\n",
    "print review_results.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Movie Rating Sentiment Classification (15 points):\n",
    "The movie review example above has a number of issues. The words are not stemmed, stopwords haven't been removed, punctuation is still included, and the classifier only uses absence and presence information for each word. Perform each of these steps below to improve the classifier, and report the change in classifier performance. \n",
    "1. Remove stopwords\n",
    "2. Remove punctuation\n",
    "3. Use the Lancaster stemmer to stem the words\n",
    "4. Instead, use the Porter stemmer to stem the words\n",
    "5. Using an SVM classifier with word counts instead of boolean flags (using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit-learn)\n",
    "6. Using the SVM again, but instead of using counts, perform the TF-IDF transformation to get the features (using [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) from scikit-learn)\n",
    "\n",
    "(Hint: I suggest using functions similar to the ones above, and specifying some boolean flags that control each step. You may need to implement new methods for train_classifier and evaluate_performance, and possibly return different features from compute_preprocessing_features and document_features based on the flags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Answer1 </h1>\n",
    "<p>1. Removed stopwords - Accuracy: 80.55%</p>\n",
    "<p>2. Removed punctuation - Accuracy: 80.05%</p>\n",
    "<p>3. Lancaster stemmer - Accuracy: 75% </p>\n",
    "<p>4. Porter stemmer - Accuracy: 78% </p>\n",
    "<p>5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Similarity\n",
    "In this section, you'll build the basic components for document retrieval and relevancy by computing [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). This measure is actually fairly straightforward to compute: it's simply the dot product of the two document vectors normalized by the document length. Commonly, the document vectors aren't simply word counts, but TF-IDF features for the documents to put greater emphasis on salient words.\n",
    "\n",
    "For example the cosine similarity, after TF-IDF normalization, of:\n",
    "   * **`a little bird`** and **`a little bird`** is 1\n",
    "   * **`a little bird`** and **`a little bird chirps`** is 0.71\n",
    "   * **`a little bird chirps`** and **`a big dog barks`** is 0 (think about why this is the case, even though they have \"a\" in common)\n",
    "\n",
    "\n",
    "You'll be using this same principle to find similar news articles in a large corpus of news data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Finding Similar Documents (15 points)\n",
    "1. Create a Text Collection from the Reuters Corpus (hint: use the function `TextCollection()`)\n",
    "2. Tokenize each document in the Reuters Corpus into words\n",
    "3. Remove punctuation\n",
    "4. Remove stop words\n",
    "5. Stem the words using PorterStemmer\n",
    "6. Compute  TF-IDF features for all of the pre-processed documents in the Reuters corpus.\n",
    "7. Write a function that computes the cosine-similarity between two documents using TF-IDF features. The function should be named cosine_sim and should take two text documents as input -- e.g., cosine_sim(text1, text2).\n",
    "8. Create a training set of fileids that contain 'train' (similar to what we did for 'pos' and 'neg' in the previous question)\n",
    "9. Find and report the most similar documents in the training set for the following fileids:\n",
    "    * test/14826\n",
    "    * test/14998\n",
    "    * test/15110\n",
    "    * test/15197\n",
    "    * test/15348"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
