{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prudhviraj Tirumanisetti\n",
    "#### EE 258 ID:01148981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Evaluation and Decision Trees\n",
    "In this assignment, we'll look at different ways of splitting data for training and testing, assessing the predictions made by data mining tools, and then explore classification using decision trees. Like Assignment 1, you will be expected to complete the assignment by answering the questions in Markdown cells and supporting your answer with corresponding code cells. \n",
    "\n",
    "Please submit your solution as two iPython notebooks (for Part 1 and Part 2). The due date for this assignment is officially 9/27/17 at 11:59pm, but this may be extended based on the feedback I receive on the SJSU Canvas survey about projects and course pace, so keep an eye out for that survey.\n",
    "\n",
    "# Part 1: Cross-validation and Evaluation Metrics (40 points)\n",
    "Let's start by checking out the dataset evaluation tools availble in scikit-learn. The [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) documentation will be helpful to review before completing this part of the assignment. We'll work with two datasets, one  generated and one real dataset, and then look at different ways of splitting that data into train and test sets, as well as computing evaluation metrics on each of the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preliminaries\n",
    "\n",
    "#Show plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets, preprocessing, cross_validation, metrics, utils, dummy, tree\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get our first dataset, the Iris data that's the default choise for demos\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Let's also generate some data. See http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification if you want to learn how this data is generated\n",
    "(gendata_d, gendata_t) = datasets.make_classification(n_samples=500, random_state=20160121)\n",
    "gendata_data = pd.DataFrame(gendata_d)\n",
    "gendata_targets = pd.DataFrame(gendata_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start with the simplest holdout procedure: a single train-test split\n",
    "[iris_train_data, iris_test_data,  iris_train_labels, iris_test_labels] = cross_validation.train_test_split(pd.DataFrame(iris.data), iris.target, test_size=0.25, random_state=20160121)\n",
    "\n",
    "# Some notes on how this works:\n",
    "## There are four outputs. The first is the dataframe containing the training data -- just the attributes. \n",
    "## The second output is the dataframe containing the testing data (attributes). The third and fourth outputs \n",
    "## are the training labels (used to train your classifier) and the testing labels (used to evaluate the classifier)\n",
    "\n",
    "## The first two arguments to the split are all the data (attributes only) and all of the labels\n",
    "## The test size is the fraction of data included in the test set, 25% in this case\n",
    "## Since we're randomly sampling data, there's a chance every student would get different results.\n",
    "##  To avoid this, we pass a random_state (20160121) to make sure the \"random\" output is the same for everyone\n",
    "\n",
    "#Let's see how big the train and test sets are:\n",
    "print iris_train_data.shape, iris_test_data.shape, iris_train_labels.shape, iris_test_labels.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Creating a simple train-test split (5 points)\n",
    "Use the `train_test_split` function (see the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split)) to generate training and testing sets for the Iris and generated data.\n",
    "1.  Generate splits with 10%, 33%, and 50% test data for both datasets (using `random_state=20160121` as a parameter) \n",
    "2.  Compute the percentage of the training labels and testing labels that belong to each class for each split. Does the distribution of labels appear to be well-matched between the train and test set?\n",
    "\n",
    "3.  Compute the descriptive statistics for each feature in the train and test set for the 10% split. Which feature has the biggest difference in mean value across datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Question 1 Answers </h1>\n",
    "<p>1. see code below </p>\n",
    "<p>2. For the IRIS dataset the distribution of the labels appears to be well-matched for the 10% and 50% split, but unbalanced for the 33% split\n",
    "<br>\n",
    "For the RAND Dataset the distribution of the labels does not appear to be as closely matched as the IRIS dataset. While the values are relatively close to each other, the test labels distribution does not match the train set label distribution</p>\n",
    "<p>3. \n",
    "For the IRIS data set the biggest difference in mean value is the 3rd feature petal width: 1.2074 -> 1.12 <br>\n",
    "For the RAND dataset the biggest difference in mean value is in attribue 8 : 0.018226 -> -0.30529\n",
    "\n",
    "<br>See cell below for better formatting</p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "2. \n",
    "\n",
    "--------- IRIS DATASET ----------\n",
    "Train data split 10%:\n",
    "0: 45 / 135 = 33.33%\n",
    "1: 45 / 135 = 33.33%\n",
    "2: 45 / 135 = 33.33%\n",
    "Test data split 10%:\n",
    "0: 5 / 15 = 33.33%\n",
    "1: 5 / 15 = 33.33%\n",
    "2: 5 / 15 = 33.33%\n",
    "\n",
    "Train data split 33%:\n",
    "0: 32 / 100 = 32.0%\n",
    "1: 34 / 100 = 34.0%\n",
    "2: 34 / 100 = 34.0%\n",
    "Test data split 33%:\n",
    "0: 18 / 50 = 36.0%\n",
    "1: 16 / 50 = 32.0%\n",
    "2: 16 / 50 = 32.0%\n",
    "\n",
    "Train data split 50%:\n",
    "0: 25 / 75 = 33.33%\n",
    "1: 25 / 75 = 33.33%\n",
    "2: 25 / 75 = 33.33%\n",
    "Test data split 50%:\n",
    "0: 25 / 75 = 33.33%\n",
    "1: 25 / 75 = 33.33%\n",
    "2: 25 / 75 = 33.33%\n",
    "\n",
    "\n",
    "--------- RAND DATASET ----------\n",
    "Train data split 10%:\n",
    "0: 226 / 450 = 50.22%\n",
    "1: 224 / 450 = 49.78%\n",
    "Test data split 10%:\n",
    "0: 23 / 50   = 46.0%\n",
    "1: 27 / 50   = 54.0%\n",
    "\n",
    "Train data split 33%:\n",
    "0: 171 / 335 = 51.04%\n",
    "1: 164 / 335 = 48.96%\n",
    "Test data split 33%:\n",
    "0: 87 / 165  = 52.73%\n",
    "1: 78 / 165  = 47.27%\n",
    "\n",
    "Train data split 50%:\n",
    "0: 129 / 250 = 51.6%\n",
    "1: 121 / 250 = 48.4%\n",
    "Test data split 50%:\n",
    "0: 120 / 250 = 49.2%\n",
    "1: 130 / 250 = 50.8%\n",
    "\n",
    "3. Compute the descriptive statistics for each feature in the train and test set for the 10% split. Which feature has the biggest difference in mean value across datasets.\n",
    "For the IRIS data set the biggest difference in mean value is the 3rd feature petal width: 1.2074 -> 1.12\n",
    "For the RAND dataset the biggest difference in mean value is in attribue 8 : 0.018226 -> -0.30529\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris CV\n",
    "[iris_train_data_10, iris_test_data_10,  iris_train_labels_10, iris_test_labels_10] = cross_validation.train_test_split(pd.DataFrame(iris.data), pd.DataFrame(iris.target), test_size=0.10, random_state=20160121)\n",
    "[iris_train_data_33, iris_test_data_33,  iris_train_labels_33, iris_test_labels_33] = cross_validation.train_test_split(pd.DataFrame(iris.data), pd.DataFrame(iris.target), test_size=0.33, random_state=20160121)\n",
    "[iris_train_data_50, iris_test_data_50,  iris_train_labels_50, iris_test_labels_50] = cross_validation.train_test_split(pd.DataFrame(iris.data), pd.DataFrame(iris.target), test_size=0.50, random_state=20160121)\n",
    "\n",
    "# Gendata CV\n",
    "[gendata_train_data_10, gendata_test_data_10,  gendata_train_labels_10, gendata_test_labels_10] = cross_validation.train_test_split(gendata_data, gendata_targets, test_size=0.10, random_state=20160121)\n",
    "[gendata_train_data_33, gendata_test_data_33,  gendata_train_labels_33, gendata_test_labels_33] = cross_validation.train_test_split(gendata_data, gendata_targets, test_size=0.33, random_state=20160121)\n",
    "[gendata_train_data_50, gendata_test_data_50,  gendata_train_labels_50, gendata_test_labels_50] = cross_validation.train_test_split(gendata_data, gendata_targets, test_size=0.50, random_state=20160121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = {'gendata_50': { 'train': gendata_train_labels_50, 'test': gendata_test_labels_50},\n",
    "                'gendata_33': { 'train': gendata_train_labels_33, 'test': gendata_test_labels_33},\n",
    "                'gendata_10': { 'train': gendata_train_labels_10, 'test': gendata_test_labels_10},\n",
    "                'iris_50': { 'train': iris_train_labels_50, 'test': iris_test_labels_50},\n",
    "                'iris_33': { 'train': iris_train_labels_33, 'test': iris_test_labels_33},\n",
    "                'iris_10': { 'train': iris_train_labels_10, 'test': iris_test_labels_10}\n",
    "               }\n",
    "for dataset in all_datasets.keys():\n",
    "    print dataset\n",
    "    print all_datasets[dataset]['train'][0].value_counts(sort=False)\n",
    "    print all_datasets[dataset]['test'][0].value_counts(sort=False)\n",
    "    print \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print iris_train_data_10.describe()\n",
    "print iris_test_data_10.describe()\n",
    "\n",
    "print gendata_train_data_10.describe()\n",
    "print gendata_test_data_10.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The next set of cross-validation split generators deal with folds - so instead of giving you a single set of outputs, they'll return an *iterator* that produces a series of train and test splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Cross-validation with k folds (10 points)\n",
    "Use the `KFold` cross-validation function to generate multiple train-test splits from each dataset\n",
    "1.   Generate 3, 5, and 10 folds for each dataset (using `random_state=20160121` as a parameter) \n",
    "2.   Compute the percentage of the training labels and testing labels that belong to each class for each set of folds. Does the distribution of labels appear to be well-matched between the train and test set?\n",
    "3.  Compute the mean value for each feature in the train and test set for each fold in the 10-fold split. Which feature has the largest difference between train and test for each of the ten folds? What is the largest difference of the averaged means for the ten folds?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question 2 Answers\n",
    "\n",
    "The percentagaes of the training labels and testing labels seem to appear well matched between train and test for all the different folds\n",
    "1,2\n",
    "Dataset: iris - nfold: 1/3\n",
    "Train data\n",
    "0    32 / 100 = 32%\n",
    "1    34 / 100 = 34%\n",
    "2    34 / 100 = 34%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    18 / 50 = 36%\n",
    "1    16 / 50 = 32%\n",
    "2    16 / 50 = 32%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 2/3\n",
    "Train data\n",
    "0    35 / 100 = 35%\n",
    "1    35 / 100 = 35%\n",
    "2    30 / 100 = 30%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    15 / 50 = 30%\n",
    "1    15 / 50 = 30%\n",
    "2    20 / 50 = 40%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 3/3\n",
    "Train data\n",
    "0    33 / 100 = 33%\n",
    "1    31 / 100 = 31%\n",
    "2    36 / 100 = 36%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    17 / 50 = 34%\n",
    "1    19 / 50 = 38%\n",
    "2    14 / 50 = 28%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 1/5\n",
    "Train data\n",
    "0    40 / 120 = 33.33%\n",
    "1    39 / 120 = 32.5%\n",
    "2    41 / 120 = 34.17\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    10 / 30 = 33.33%\n",
    "1    11 / 30 = 36.67%\n",
    "2     9 / 30 = 30%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 2/5\n",
    "Train data\n",
    "0    40 / 120 = 33.33%\n",
    "1    39 / 120 = 32.5%\n",
    "2    41 / 120 = 34.17%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    10 / 30 = 33.33%\n",
    "1    11 / 30 = 36.67%\n",
    "2     9 / 30 = 30%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 3/5\n",
    "Train data\n",
    "0    42 / 120 = 35%\n",
    "1    42 / 120 = 35%\n",
    "2    36 / 120 = 30%\n",
    "Name: 0, dtype: int64\n",
    "Test data \n",
    "0     8 / 30 = 26.67%\n",
    "1     8 / 30 = 26.67%\n",
    "2    14 / 30 = 46.67%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 4/5\n",
    "Train data\n",
    "0    39 / 120 = 32.5%\n",
    "1    42 / 120 = 35%\n",
    "2    39 / 120 = 32.5%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    11 / 30 = 36.67%\n",
    "1     8 / 30 = 26.67%\n",
    "2    11 / 30 = 36.67%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 5/5\n",
    "Train data\n",
    "0    39 / 120 = 32.5%\n",
    "1    38 / 120 = 31.67%\n",
    "2    43 / 120 = 35.83%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    11 / 30 = 36.67%\n",
    "1    12 / 30 = 40%\n",
    "2     7 / 30 = 23.33%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 1/10\n",
    "Train data\n",
    "0    45 / 135 = 33.33%\n",
    "1    45 / 135 = 33.33%\n",
    "2    45 / 135 = 33.33%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    5 / 15 = 33.33%\n",
    "1    5 / 15 = 33.33%\n",
    "2    5 / 15 = 33.33%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 2/10\n",
    "Train data\n",
    "0    45 / 135 = 33.33%\n",
    "1    44 / 135 = 32.59%\n",
    "2    46 / 135 = 34.08%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    5 / 15 = 33.33%\n",
    "1    6 / 15 = 40%\n",
    "2    4 / 15 = 26.67%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 3/10\n",
    "Train data\n",
    "0    43 / 135 = 31.85%\n",
    "1    47 / 135 = 34.81%\n",
    "2    45 / 135 = 33.33%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    7 / 15 = 46.67%\n",
    "1    3 / 15 = 20%\n",
    "2    5 / 15 = 33.33%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 4/10\n",
    "Train data\n",
    "0    47 / 135 = 34.81%\n",
    "1    42 / 135 = 31.11%\n",
    "2    46 / 135 = 34.08%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    3 / 15 = 20%\n",
    "1    8 / 15 = 53.33%\n",
    "2    4 / 15 = 26.67%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 5/10\n",
    "Train data\n",
    "0    45 / 135 = 33.33%\n",
    "1    47 / 135 = 34.81%\n",
    "2    43 / 135 = 31.86%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    5 / 15 = 33.33%\n",
    "1    3 / 15 = 20%\n",
    "2    7 / 15 = 46.67%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 6/10\n",
    "Train data\n",
    "0    47 / 135 = 34.81%\n",
    "1    45 / 135 = 33.33%\n",
    "2    43 / 135 = 31.86%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    3 / 15 = 20%\n",
    "1    5 / 15 = 33.33%\n",
    "2    7 / 15 = 46.67%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 7/10\n",
    "Train data\n",
    "0    44 / 135 = 32.59%\n",
    "1    48 / 135 = 35.55%\n",
    "2    43 / 135 = 31.85%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    6 / 15 = 40%\n",
    "1    2 / 15 = 13.33%\n",
    "2    7 / 15 = 46.67%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 8/10\n",
    "Train data\n",
    "0    45 / 135 = 33.33%\n",
    "1    44 / 135 = 32.59%\n",
    "2    46 / 135 = 34.08%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    5 / 15 = 33.33%\n",
    "1    6 / 15 = 40%\n",
    "2    4 / 15 = 26.67%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 9/10\n",
    "Train data\n",
    "0    43 / 135 = 31.85%\n",
    "1    43 / 135 = 31.85%\n",
    "2    49 / 135 = 36.30%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    7 / 15 = 46.67%\n",
    "1    7 / 15 = 46.67%\n",
    "2    1 / 15 = 6.67%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: iris - nfold: 10/10\n",
    "Train data\n",
    "0    46 / 135 = 34.07%\n",
    "1    45 / 135 = 33.33%\n",
    "2    44 / 135 = 32.59%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    4 / 15 = 26.67%\n",
    "1    5 / 15 = 33.33%\n",
    "2    6 / 15 = 40%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 1/3\n",
    "Train data\n",
    "0    170 / 333 = 51.05%\n",
    "1    163 / 333 = 48.95%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    79 / 167 = 47.31%\n",
    "1    88 / 167 = 52.69%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 2/3\n",
    "Train data\n",
    "0    168 / 333 = 50.45%\n",
    "1    165 / 333 = 49.55%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    81 / 167 = 48.50%\n",
    "1    86 / 167 = 51.50%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 3/3\n",
    "Train data\n",
    "0    160 / 333 = 48.05%\n",
    "1    174 / 333 = 51.95%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    89 / 167 = 53.29%\n",
    "1    77 / 167 = 46.61%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 1/5\n",
    "Train data\n",
    "0    202 / 400 = 50.5%\n",
    "1    198 / 400 = 49.5%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    47 / 100 = 47%\n",
    "1    53 / 100 = 53%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 2/5\n",
    "Train data\n",
    "0    204 / 400 = 51%\n",
    "1    196 / 400 = 49%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    45 / 100 = 45%\n",
    "1    55 / 100 = 55%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 3/5\n",
    "Train data\n",
    "0    198 / 400 = 49.5%\n",
    "1    202 / 400 = 50.5%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    51 / 100 = 51%\n",
    "1    49 / 100 = 49%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 4/5\n",
    "Train data\n",
    "0    195 / 400 = 48.75%\n",
    "1    205 / 400 = 51.25%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    54 / 100 = 54%\n",
    "1    46 / 100 = 46%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 5/5\n",
    "Train data\n",
    "0    197 / 400 = 49.25%\n",
    "1    203 / 400 = 51.65%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    52 / 100 = 52%\n",
    "1    48 / 100 = 48%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 1/10\n",
    "Train data\n",
    "0    226 / 450 = 50.22%\n",
    "1    224 / 450 = 49.78%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    23 / 50 = 46%\n",
    "1    27 / 50 = 54%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 2/10\n",
    "Train data\n",
    "0    225 / 450 = 50%\n",
    "1    225 / 450 = 50%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    24 / 50 = 48%\n",
    "1    26 / 50 = 52%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 3/10\n",
    "Train data\n",
    "0    226 / 450 = 50.22%\n",
    "1    224 / 450 = 49.78%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    23 / 50 = 46%\n",
    "1    27 / 50 = 54%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 4/10\n",
    "Train data\n",
    "0    227 / 450 = 50.44%\n",
    "1    223 / 450 = 49.56%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    22 / 50 = 44%\n",
    "1    28 / 50 = 56%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 5/10\n",
    "Train data\n",
    "0    221 / 450 = 49.11%\n",
    "1    229 / 450 = 50.89%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    28 / 50 = 56%\n",
    "1    22 / 50 = 44%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 6/10\n",
    "Train data\n",
    "0    226 / 450 = 50.22%\n",
    "1    224 / 450 = 49.78%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    23 / 50 = 46%\n",
    "1    27 / 50 = 54%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 7/10\n",
    "Train data\n",
    "0    224 / 450 = 49.78%\n",
    "1    226 / 450 = 50.22%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    25 / 50 = 50%\n",
    "1    25 / 50 = 50%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 8/10\n",
    "Train data\n",
    "0    220 / 450 = 48.89%\n",
    "1    230 / 450 = 51.11%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    29 / 50 = 58%\n",
    "1    21 / 50 = 42%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 9/10\n",
    "Train data\n",
    "0    224 / 450 = 49.78%\n",
    "1    226 / 450 = 50.22%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    25 / 50 = 50%\n",
    "1    25 / 50 = 50%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: gen - nfold: 10/10\n",
    "Train data\n",
    "0    222 / 450 = 49.33%\n",
    "1    228 / 450 = 50.67%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    27 / 50 = 54%\n",
    "1    23 / 50 = 46%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "3. For IRIS dataset the largest difference between train and test mean is Fold 6 and featre 3 (petal length) with difference of 0.69037\n",
    "Largest difference in the averaged means for the ten folds is in feature 2, sepal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "    \n",
    "all_datasets = {'iris': {'data': pd.DataFrame(iris.data), 'labels': pd.DataFrame(iris.target)}, \n",
    "            'gen': { 'data': gendata_data, 'labels': gendata_targets} }\n",
    "        \n",
    "for dataset in all_datasets.keys():\n",
    "    for nfolds in [3, 5, 10]:\n",
    "        kfolds = cross_validation.KFold(all_datasets[dataset]['labels'].size, n_folds=nfolds, shuffle=True, random_state=20160121)\n",
    "        fold = 0\n",
    "        for train, test in kfolds:\n",
    "                fold += 1\n",
    "#                 print (\"%s %s\" % (train,test))\n",
    "                if (dataset == 'iris'):\n",
    "                    train_fold = pd.DataFrame(iris.target[train])\n",
    "                    test_fold = pd.DataFrame(iris.target[test])\n",
    "                else:\n",
    "                    train_fold = pd.DataFrame(gendata_t[train])\n",
    "                    test_fold = pd.DataFrame(gendata_t[test])         \n",
    "                    \n",
    "                print \"Dataset: \" + dataset + \" - nfold: \" + str(fold) + \"/\" + str(nfolds)\n",
    "                print \"Train data\"\n",
    "                print train_fold[0].value_counts(sort=False)\n",
    "                print \"Test data\"\n",
    "                print test_fold[0].value_counts(sort=False)\n",
    "                print \"\\n\"\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "    \n",
    "all_datasets = {'iris': {'data': pd.DataFrame(iris.data), 'labels': pd.DataFrame(iris.target)}, \n",
    "            'gen': { 'data': gendata_data, 'labels': gendata_targets} }\n",
    "        \n",
    "for dataset in all_datasets.keys():\n",
    "    for nfolds in [10]:\n",
    "        kfolds = cross_validation.KFold(all_datasets[dataset]['labels'].size, n_folds=nfolds, shuffle=True, random_state=20160121)\n",
    "        fold = 0\n",
    "        for train, test in kfolds:\n",
    "                fold += 1\n",
    "#                 print (\"%s %s\" % (train,test))\n",
    "                if (dataset == 'iris'):\n",
    "                    train_fold = pd.DataFrame(iris.data[train])\n",
    "                    test_fold = pd.DataFrame(iris.data[test])\n",
    "                else:\n",
    "                    train_fold = pd.DataFrame(gendata_d[train])\n",
    "                    test_fold = pd.DataFrame(gendata_d[test])         \n",
    "                    \n",
    "                print \"Dataset: \" + dataset + \" - nfold: \" + str(fold) + \"/\" + str(nfolds)\n",
    "                print \"Train data\"\n",
    "                print train_fold.describe()\n",
    "                print \"Test data\"\n",
    "                print test_fold.describe()\n",
    "                print \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sometimes it's important to ensure you get equal samples of each class, particularly when the classes are imbalanced. Let's generate some data with a label skew and see what happens in cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make some skewed data where the instances with label 1 are very rare:\n",
    "(skew_gendata_d, skew_gendata_t) = datasets.make_classification(n_samples=500, weights=[0.9], random_state=20160121)\n",
    "skew_gendata_data = pd.DataFrame(skew_gendata_d)\n",
    "skew_gendata_targets = pd.DataFrame(skew_gendata_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Stratified folds (5 points)\n",
    "1.  Use the `KFold` cross-validation method and generate 10 folds (using `random_state=20160121`). What is the range of percentages for label 1 across the folds?\n",
    "2.  Use the `StratifiedKFold` cross-validation method and generate 10 folds (using `random_state=20160121`). What is the range of percentages for label 1 across the folds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Question 3 Answer </h1>\n",
    "<p>1. The range of percentages for label 1 is from 10.22% to 11.11% for the train data and \n",
    "from 8% to 16% for the test data</p>\n",
    "<p>2. The range of percentages for label 1 is \n",
    "from 10.69% to 10.86% for the train data \n",
    "from 10% to 11.76% for the test data </p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.\n",
    "Dataset: skew - nfold: 1/10\n",
    "Train data\n",
    "0    401 / 450 = 89.1%\n",
    "1     49 / 450 = 10.9%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    45 / 50 = 90%\n",
    "1     5 / 50 = 10%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 2/10\n",
    "Train data\n",
    "0    403 / 450 = 89.56%\n",
    "1     47 / 450 = 10.44%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    43 / 50 = 86%\n",
    "1     7 / 50 = 14%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 3/10\n",
    "Train data\n",
    "0    403 / 450 = 89.56%\n",
    "1     47 / 450 = 10.44%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    43 / 50 = 86%\n",
    "1     7 / 50 = 14%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 4/10\n",
    "Train data\n",
    "0    400 / 450 = 88.89%\n",
    "1     50 / 450 = 11.11%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    46 / 50 = 92%\n",
    "1     4 / 50 = 8%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 5/10\n",
    "Train data\n",
    "0    400 / 450 = 88.89%\n",
    "1     50 / 450 = 11.11%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    46 / 50 = 92%\n",
    "1     4 / 50 = 8%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 6/10\n",
    "Train data\n",
    "0    404 / 450 = 89.78%\n",
    "1     46 / 450 = 10.22%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    42 / 50 = 84%\n",
    "1     8 / 50 = 16%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 7/10\n",
    "Train data\n",
    "0    403 / 450 = 89.56%\n",
    "1     47 / 450 = 10.44%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    43 / 50 = 86%\n",
    "1     7 / 50 = 14%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 8/10\n",
    "Train data\n",
    "0    400 / 450 = 88.89%\n",
    "1     50 / 450 = 11.11$\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    46 / 50 = 92%\n",
    "1     4 / 50 = 8%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 9/10\n",
    "Train data\n",
    "0    400 / 450 = 88.89%\n",
    "1     50 / 450 = 11.11%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    46 / 50 = 92%\n",
    "1     4 / 50 = 8%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 10/10\n",
    "Train data\n",
    "0    400 / 450 = 88.89%\n",
    "1     50 / 450 = 11.11%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    46 / 50 = 92%\n",
    "1     4 / 50 = 8%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "What is the range of percentages for label 1 across the folds?\n",
    "The range of percentages for label 1 is \n",
    "from 10.22% to 11.11% for the train data and\n",
    "from 8% to 16% for the test data\n",
    "\n",
    "\n",
    "2.\n",
    "ataset: skew - nfold: 1/10\n",
    "Train data\n",
    "0    401 / 449 = 89.31%\n",
    "1     48 / 449 = 10.69%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    45 / 51 = 88.24%\n",
    "1     6 / 51 = 11.76%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 2/10\n",
    "Train data\n",
    "0    401 / 449 = 89.31%\n",
    "1     48 / 449 = 10.69%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    45 / 51 = 88.24%\n",
    "1     6 / 51 = 11.76%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 3/10\n",
    "Train data\n",
    "0    401 / 449 = 89.31%\n",
    "1     48 / 449 = 10.69%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    45 / 51 = 88.24%\n",
    "1     6 / 51 = 11.76%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 4/10\n",
    "Train data\n",
    "0    401 / 449 = 89.31%\n",
    "1     48 / 449 = 10.69%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    45 / 51 = 88.24%\n",
    "1     6 / 51 = 11.76%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 5/10\n",
    "Train data\n",
    "0    401 / 449 = 89.31%\n",
    "1     48 / 449 = 10.69%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    45 / 51 = 88.24%\n",
    "1     6 / 51 = 11.76%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 6/10\n",
    "Train data\n",
    "0    401 / 450 = 89.11%\n",
    "1     49 / 450 = 10.89%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    45 / 50 = 90%\n",
    "1     5 / 50 = 10%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 7/10\n",
    "Train data\n",
    "0    402 / 451 = 89.14%\n",
    "1     49 / 451 = 10.86%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    44 / 49 = 89.80%\n",
    "1     5 / 49 = 10.20%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 8/10\n",
    "Train data\n",
    "0    402 / 451 = 89.14%\n",
    "1     49 / 451 = 10.86%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    44 / 49 = 89.8%\n",
    "1     5 / 49 = 10.2%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 9/10\n",
    "Train data\n",
    "0    402 / 451 = 89.14%\n",
    "1     49 / 451 = 10.86%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    44 / 49 = 89.8%\n",
    "1     5 / 49 = 10.2%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Dataset: skew - nfold: 10/10\n",
    "Train data\n",
    "0    402 / 451 = 89.14%\n",
    "1     49 / 451 = 10.86%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    44 / 49 = 89.8%\n",
    "1     5 / 49 = 10.2%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "What is the range of percentages for label 1 across the folds?\n",
    "The range of percentages for label 1 is \n",
    "from 10.69% to 10.86% for the train data \n",
    "from 10% to 11.76% for the test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = {'skew': { 'data': skew_gendata_data, 'labels': skew_gendata_targets} }\n",
    "        \n",
    "for dataset in all_datasets.keys():\n",
    "    for nfolds in [10]:\n",
    "        kfolds = cross_validation.KFold(all_datasets[dataset]['labels'].size, n_folds=nfolds, shuffle=True, random_state=20160121)\n",
    "        fold = 0\n",
    "        for train, test in kfolds:\n",
    "            fold += 1\n",
    "            train_fold = pd.DataFrame(skew_gendata_t[train])\n",
    "            test_fold = pd.DataFrame(skew_gendata_t[test])\n",
    "\n",
    "            print \"Dataset: \" + dataset + \" - nfold: \" + str(fold) + \"/\" + str(nfolds)\n",
    "            print \"Train data\"\n",
    "            print train_fold[0].value_counts(sort=False)\n",
    "            print \"Test data\"\n",
    "            print test_fold[0].value_counts(sort=False)\n",
    "            print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print skew_gendata_targets.shape\n",
    "print np.reshape(skew_gendata_targets.values,[500,]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the StratifiedKFold cross-validation method and generate 10 folds (using random_state=20160121). \n",
    "# What is the range of percentages for label 1 across the folds?\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "kfolds = cross_validation.StratifiedKFold(np.reshape(skew_gendata_targets.values,[500,]), n_folds=10, shuffle=True, random_state=20160121)\n",
    "fold = 0\n",
    "for train, test in kfolds:\n",
    "    fold += 1\n",
    "    train_fold = pd.DataFrame(skew_gendata_t[train])\n",
    "    test_fold = pd.DataFrame(skew_gendata_t[test])\n",
    "\n",
    "    print \"Dataset: \" + dataset + \" - nfold: \" + str(fold) + \"/\" + str(nfolds)\n",
    "    print \"Train data\"\n",
    "    print train_fold[0].value_counts(sort=False)\n",
    "    print \"Test data\"\n",
    "    print test_fold[0].value_counts(sort=False)\n",
    "    print \"\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One of the techniques we discussed was the bootstrap: a method to get a training set the size of our dataset while still getting useful training data. You can use the [resample](http://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html) method from sklearn's util module to implement the bootstrap. Resample will generate a training set for you and, by default, make it the same size as the dataset. The only remaining challenge is figuring out what to put in the test set. You can use the `index` attribute of a pandas DataFrame to help you there..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: The Bootstrap (10 points)\n",
    "1.  Generate a bootstrap based train-test split for the three datasets (gendata, skew_gendata, and iris)  using the `resample` method. Remember to set `random_state=20160121`\n",
    "2.  Compute the label proportions in the train and test sets. How do they compare?\n",
    "3.  Compute the descriptive statistics for each of the attributes. Which attribute has the largest difference in mean across the train and test sets?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. Gendata label proportions are not great, but they are close enough to be considered good enough\n",
    "3. 15th attribute has the largest difference in mean across the train and test sets\n",
    "\n",
    "Train data\n",
    "0    259 / 500 = 52%\n",
    "1    241 / 500 = 48%\n",
    "Name: 0, dtype: int64\n",
    "Test data\n",
    "0    80 / 175 = 46%\n",
    "1    95 / 175 = 54%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "GENDATA\n",
    "\n",
    "               0           1           2           3           4           5   \\\n",
    "count  500.000000  500.000000  500.000000  500.000000  500.000000  500.000000   \n",
    "mean    -0.003139   -0.013427    0.021906    0.035423    0.012670    0.096852   \n",
    "std      0.991457    1.020868    1.018835    0.981776    1.041781    1.003704   \n",
    "min     -2.421082   -2.433603   -2.538242   -2.518441   -3.223148   -2.536563   \n",
    "25%     -0.720644   -0.757001   -0.615010   -0.571672   -0.740107   -0.535868   \n",
    "50%     -0.013046   -0.070751   -0.000330    0.008836    0.047413    0.171539   \n",
    "75%      0.661775    0.620804    0.679261    0.723654    0.644627    0.862462   \n",
    "max      3.594499    3.039572    3.003953    2.619359    2.454954    3.432993   \n",
    "\n",
    "               6           7           8           9           10          11  \\\n",
    "count  500.000000  500.000000  500.000000  500.000000  500.000000  500.000000   \n",
    "mean    -0.023746   -0.068885    0.042949    0.004360    0.024588   -0.075621   \n",
    "std      1.180548    1.008090    1.026896    0.973219    1.174396    0.979879   \n",
    "min     -3.326552   -2.588259   -2.807253   -2.690036   -3.315302   -3.102408   \n",
    "25%     -1.023971   -0.783844   -0.652606   -0.718984   -0.843480   -0.738850   \n",
    "50%     -0.103775   -0.064137    0.099790    0.030052    0.078181   -0.085270   \n",
    "75%      0.929908    0.643163    0.803920    0.658938    1.040590    0.649183   \n",
    "max      3.023143    2.731177    2.949241    3.135951    2.544278    2.664877   \n",
    "\n",
    "               12          13          14          15          16          17  \\\n",
    "count  500.000000  500.000000  500.000000  500.000000  500.000000  500.000000   \n",
    "mean    -0.024864   -0.043657    0.075582    0.058304   -0.033319   -0.027178   \n",
    "std      0.961753    1.191530    0.933707    0.965128    0.975602    1.013068   \n",
    "min     -3.109823   -3.210675   -2.261836   -3.404818   -2.450816   -2.883941   \n",
    "25%     -0.624986   -0.998630   -0.612217   -0.643052   -0.759561   -0.800636   \n",
    "50%     -0.091031   -0.229074    0.085333    0.101073   -0.032146    0.063717   \n",
    "75%      0.695669    0.955272    0.796331    0.798477    0.619367    0.642004   \n",
    "max      2.706916    3.233625    2.760497    2.630985    3.094925    3.264062   \n",
    "\n",
    "               18          19  \n",
    "count  500.000000  500.000000  \n",
    "mean     0.057141   -0.014074  \n",
    "std      1.032833    1.002647  \n",
    "min     -3.389571   -3.335489  \n",
    "25%     -0.593457   -0.728227  \n",
    "50%      0.118198   -0.007640  \n",
    "75%      0.708951    0.590290  \n",
    "max      2.811399    3.498259  \n",
    "               0           1           2           3           4           5   \\\n",
    "count  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000   \n",
    "mean     0.005619    0.093439    0.032655    0.026498    0.168349    0.051515   \n",
    "std      1.050662    0.974157    0.855438    1.087417    1.041658    1.011249   \n",
    "min     -2.515058   -1.888726   -2.647789   -2.530270   -2.538309   -2.966130   \n",
    "25%     -0.647823   -0.514815   -0.510086   -0.699466   -0.490877   -0.511488   \n",
    "50%     -0.029770    0.019143    0.069402    0.075448    0.123224    0.088291   \n",
    "75%      0.725104    0.621529    0.613381    0.740656    0.841647    0.691024   \n",
    "max      2.370750    3.012955    2.391449    2.479612    2.712419    2.421206   \n",
    "\n",
    "               6           7           8           9           10          11  \\\n",
    "count  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000   \n",
    "mean    -0.061467    0.029711   -0.075015    0.022355   -0.108282   -0.041691   \n",
    "std      1.202664    0.999409    0.995259    1.080562    1.138179    0.892856   \n",
    "min     -2.798720   -2.736625   -3.060349   -3.021813   -3.265268   -2.250563   \n",
    "25%     -1.048574   -0.599275   -0.683936   -0.800349   -0.823582   -0.600243   \n",
    "50%     -0.260886    0.060532   -0.100929   -0.012445   -0.106142   -0.130859   \n",
    "75%      0.859118    0.601171    0.469951    0.770578    0.702921    0.552187   \n",
    "max      2.639439    2.366253    3.080764    2.726222    1.985425    1.873812   \n",
    "\n",
    "               12          13          14          15          16          17  \\\n",
    "count  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000   \n",
    "mean     0.006369    0.089047    0.150264   -0.100891    0.078559   -0.122936   \n",
    "std      1.015259    1.247470    1.010591    0.898343    1.096469    0.907329   \n",
    "min     -2.451936   -3.183078   -3.239524   -2.446034   -3.068310   -2.852202   \n",
    "25%     -0.731692   -0.912062   -0.470415   -0.725092   -0.759722   -0.683651   \n",
    "50%      0.010469    0.106732    0.208500   -0.067307    0.081378   -0.116021   \n",
    "75%      0.754626    1.068886    0.775779    0.522074    0.869717    0.439826   \n",
    "max      2.373247    3.303948    3.446098    2.253107    2.961835    2.598636   \n",
    "\n",
    "               18          19  \n",
    "count  175.000000  175.000000  \n",
    "mean    -0.087160    0.027925  \n",
    "std      0.969802    1.060347  \n",
    "min     -2.479270   -3.110583  \n",
    "25%     -0.762449   -0.576496  \n",
    "50%      0.043538    0.031946  \n",
    "75%      0.585937    0.723740  \n",
    "max      2.477594    2.865515 \n",
    "\n",
    "-----------------------------\n",
    "SKEW\n",
    "\n",
    "Skew data label proportions are not very good, especially the under representation of label 0 in the test set. It's under represented which means that our model might not make the right predictions accordingly\n",
    "15th attribute has the largest difference in mean across the train and test sets\n",
    "\n",
    "Train data Skew\n",
    "0    458 / 500 = 92%\n",
    "1     42 / 500 = 8%\n",
    "Name: 0, dtype: int64\n",
    "Test data Skew\n",
    "0    149 / 175 = 85%\n",
    "1     26 / 175 = 15%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "               0           1           2           3           4           5   \\\n",
    "count  500.000000  500.000000  500.000000  500.000000  500.000000  500.000000   \n",
    "mean    -0.003139   -0.536252    0.021906    0.035423    0.012670    0.096852   \n",
    "std      0.991457    0.767778    1.018835    0.981776    1.041781    1.003704   \n",
    "min     -2.421082   -2.433603   -2.538242   -2.518441   -3.223148   -2.536563   \n",
    "25%     -0.720644   -1.192733   -0.615010   -0.571672   -0.740107   -0.535868   \n",
    "50%     -0.013046   -0.456621   -0.000330    0.008836    0.047413    0.171539   \n",
    "75%      0.661775    0.031560    0.679261    0.723654    0.644627    0.862462   \n",
    "max      3.594499    1.763571    3.003953    2.619359    2.454954    3.432993   \n",
    "\n",
    "               6           7           8           9           10          11  \\\n",
    "count  500.000000  500.000000  500.000000  500.000000  500.000000  500.000000   \n",
    "mean     0.026556   -0.068885    0.042949    0.004360    0.711829   -0.075621   \n",
    "std      1.088736    1.008090    1.026896    0.973219    0.806941    0.979879   \n",
    "min     -1.911546   -2.588259   -2.807253   -2.690036   -1.990374   -3.102408   \n",
    "25%     -0.928071   -0.783844   -0.652606   -0.718984    0.184751   -0.738850   \n",
    "50%     -0.060348   -0.064137    0.099790    0.030052    0.844790   -0.085270   \n",
    "75%      0.949863    0.643163    0.803920    0.658938    1.347421    0.649183   \n",
    "max      3.023143    2.731177    2.949241    3.135951    2.544278    2.664877   \n",
    "\n",
    "               12          13          14          15          16          17  \\\n",
    "count  500.000000  500.000000  500.000000  500.000000  500.000000  500.000000   \n",
    "mean    -0.024864   -0.820048    0.075582    0.058304   -0.033319   -0.027178   \n",
    "std      0.961753    0.720215    0.933707    0.965128    0.975602    1.013068   \n",
    "min     -3.109823   -3.210675   -2.261836   -3.404818   -2.450816   -2.883941   \n",
    "25%     -0.624986   -1.133534   -0.612217   -0.643052   -0.759561   -0.800636   \n",
    "50%     -0.091031   -0.955740    0.085333    0.101073   -0.032146    0.063717   \n",
    "75%      0.695669   -0.698445    0.796331    0.798477    0.619367    0.642004   \n",
    "max      2.706916    1.877995    2.760497    2.630985    3.094925    3.264062   \n",
    "\n",
    "               18          19  \n",
    "count  500.000000  500.000000  \n",
    "mean     0.057141   -0.014074  \n",
    "std      1.032833    1.002647  \n",
    "min     -3.389571   -3.335489  \n",
    "25%     -0.593457   -0.728227  \n",
    "50%      0.118198   -0.007640  \n",
    "75%      0.708951    0.590290  \n",
    "max      2.811399    3.498259  \n",
    "               0           1           2           3           4           5   \\\n",
    "count  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000   \n",
    "mean     0.005619   -0.398621    0.032655    0.026498    0.168349    0.051515   \n",
    "std      1.050662    0.812624    0.855438    1.087417    1.041658    1.011249   \n",
    "min     -2.515058   -2.046301   -2.647789   -2.530270   -2.538309   -2.966130   \n",
    "25%     -0.647823   -1.060373   -0.510086   -0.699466   -0.490877   -0.511488   \n",
    "50%     -0.029770   -0.362135    0.069402    0.075448    0.123224    0.088291   \n",
    "75%      0.725104    0.147743    0.613381    0.740656    0.841647    0.691024   \n",
    "max      2.370750    2.245763    2.391449    2.479612    2.712419    2.421206   \n",
    "\n",
    "               6           7           8           9           10          11  \\\n",
    "count  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000   \n",
    "mean    -0.094514    0.029711   -0.075015    0.022355    0.560796   -0.041691   \n",
    "std      1.108151    0.999409    0.995259    1.080562    0.919052    0.892856   \n",
    "min     -2.272284   -2.736625   -3.060349   -3.021813   -2.552006   -2.250563   \n",
    "25%     -1.027302   -0.599275   -0.683936   -0.800349   -0.074823   -0.600243   \n",
    "50%     -0.336453    0.060532   -0.100929   -0.012445    0.721716   -0.130859   \n",
    "75%      0.829838    0.601171    0.469951    0.770578    1.240600    0.552187   \n",
    "max      2.626074    2.366253    3.080764    2.726222    2.017827    1.873812   \n",
    "\n",
    "               12          13          14          15          16          17  \\\n",
    "count  175.000000  175.000000  175.000000  175.000000  175.000000  175.000000   \n",
    "mean     0.006369   -0.717808    0.150264   -0.100891    0.078559   -0.122936   \n",
    "std      1.015259    0.992545    1.010591    0.898343    1.096469    0.907329   \n",
    "min     -2.451936   -3.183078   -3.239524   -2.446034   -3.068310   -2.852202   \n",
    "25%     -0.731692   -1.193541   -0.470415   -0.725092   -0.759722   -0.683651   \n",
    "50%      0.010469   -0.946576    0.208500   -0.067307    0.081378   -0.116021   \n",
    "75%      0.754626   -0.325370    0.775779    0.522074    0.869717    0.439826   \n",
    "max      2.373247    3.303948    3.446098    2.253107    2.961835    2.598636   \n",
    "\n",
    "               18          19  \n",
    "count  175.000000  175.000000  \n",
    "mean    -0.087160    0.027925  \n",
    "std      0.969802    1.060347  \n",
    "min     -2.479270   -3.110583  \n",
    "25%     -0.762449   -0.576496  \n",
    "50%      0.043538    0.031946  \n",
    "75%      0.585937    0.723740  \n",
    "max      2.477594    2.865515 \n",
    "\n",
    "\n",
    "-----------------------------\n",
    "IRIS\n",
    "\n",
    "Iris data label proportions seem good. Better than the other two datasets\n",
    "2nd attribute has the largest difference in mean across the train and test sets\n",
    "\n",
    "\n",
    "Train data Iris \n",
    "0    54 / 150 = 36%\n",
    "1    50 / 150 = 33%\n",
    "2    46 / 150 = 31%\n",
    "Name: 0, dtype: int64\n",
    "Test data Iris\n",
    "0    20 / 54 = 37%\n",
    "1    16 / 54 = 30%\n",
    "2    18 / 54 = 33.%\n",
    "Name: 0, dtype: int64\n",
    "\n",
    "\n",
    "Train data Iris\n",
    "                0           1           2           3\n",
    "count  150.000000  150.000000  150.000000  150.000000\n",
    "mean     5.815333    3.018000    3.649333    1.128667\n",
    "std      0.782590    0.398162    1.731498    0.748810\n",
    "min      4.400000    2.000000    1.000000    0.100000\n",
    "25%      5.100000    2.800000    1.500000    0.200000\n",
    "50%      5.800000    3.000000    4.350000    1.300000\n",
    "75%      6.375000    3.300000    5.000000    1.800000\n",
    "max      7.700000    4.400000    6.900000    2.500000\n",
    "\n",
    "Test data Iris\n",
    "               0          1          2          3\n",
    "count  54.000000  54.000000  54.000000  54.000000\n",
    "mean    5.783333   3.114815   3.618519   1.157407\n",
    "std     0.855559   0.459864   1.845042   0.802261\n",
    "min     4.300000   2.200000   1.100000   0.100000\n",
    "25%     5.100000   2.800000   1.500000   0.225000\n",
    "50%     5.600000   3.000000   4.050000   1.300000\n",
    "75%     6.300000   3.475000   5.100000   1.800000\n",
    "max     7.900000   4.200000   6.700000   2.500000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "boot_gendata_train = resample(gendata_targets, random_state=20160121)\n",
    "boot_gendata_test = gendata_targets.loc[~gendata_targets.index.isin(list(boot_gendata_train.index.values))]\n",
    "\n",
    "# print boot_gendata_train\n",
    "# print boot_gendata_test\n",
    "\n",
    "print \"Train data\"\n",
    "print boot_gendata_train[0].value_counts(sort=False)\n",
    "print \"Test data\"\n",
    "print boot_gendata_test[0].value_counts(sort=False)\n",
    "print \"\\n\"\n",
    "\n",
    "boot_gendata_train = resample(gendata_data, random_state=20160121)\n",
    "boot_gendata_test = gendata_data.loc[~gendata_data.index.isin(list(boot_gendata_train.index.values))]\n",
    "\n",
    "print boot_gendata_train.describe()\n",
    "print boot_gendata_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "boot_skew_gendata_train = resample(skew_gendata_targets, random_state=20160121)\n",
    "boot_skew_gendata_test = skew_gendata_targets.loc[~skew_gendata_targets.index.isin(list(boot_skew_gendata_train.index.values))]\n",
    "\n",
    "# print boot_skew_gendata_train.shape\n",
    "# print boot_skew_gendata_test.shape\n",
    "\n",
    "print \"Train data Skew\"\n",
    "print boot_skew_gendata_train[0].value_counts(sort=False)\n",
    "print \"Test data Skew\"\n",
    "print boot_skew_gendata_test[0].value_counts(sort=False)\n",
    "print \"\\n\"\n",
    "\n",
    "boot_skew_gendata_train = resample(skew_gendata_data, random_state=20160121)\n",
    "boot_skew_gendata_test = skew_gendata_data.loc[~skew_gendata_data.index.isin(list(boot_skew_gendata_train.index.values))]\n",
    "\n",
    "print boot_skew_gendata_train.describe()\n",
    "print boot_skew_gendata_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "iris_data = pd.DataFrame(iris.target)\n",
    "boot_iris_train = resample(iris_data, random_state=20160121)\n",
    "\n",
    "boot_iris_test = iris_data.loc[~iris_data.index.isin(list(boot_iris_train.index.values))]\n",
    "\n",
    "print \"Train data Iris \"\n",
    "print boot_iris_train[0].value_counts(sort=False)\n",
    "print \"Test data Iris\"\n",
    "print boot_iris_test[0].value_counts(sort=False)\n",
    "print \"\\n\"\n",
    "\n",
    "iris_data = pd.DataFrame(iris.data)\n",
    "boot_iris_train = resample(iris_data, random_state=20160121)\n",
    "\n",
    "boot_iris_test = iris_data.loc[~iris_data.index.isin(list(boot_iris_train.index.values))]\n",
    "\n",
    "print \"Train data Iris\"\n",
    "print boot_iris_train.describe()\n",
    "print \"\\nTest data Iris\"\n",
    "print boot_iris_test.describe()\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Models\n",
    "Now that we've looked at various means of splitting our data, we can explore the performance metrics used to evaluate our results. You can find a full list of these metrics documented in the [metrics documentation](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics). Since it's not very interesting to look at these metrics in isolation, we can generate some classification output for our data and see how the different methods perform.\n",
    "\n",
    "One of the other topics we discussed was comparing schemes or methods to decide if the differences between them were statistically meaningful. We evaluate the statistical significance of these results using t-tests. The t-test isn't in scikit-learn, but in a related module, `scipy`. You can read about all of the statistical tests in the [scipy stats documentation](http://docs.scipy.org/doc/scipy/reference/stats.html) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the dummy classifier on the Iris data:\n",
    "dummy_classifier_iris = dummy.DummyClassifier();\n",
    "dummy_classifier_iris.fit(iris_train_data, iris_train_labels);\n",
    "dummy_iris_predictions = dummy_classifier_iris.predict(iris_test_data);\n",
    "dummy_iris_predictions_proba = dummy_classifier_iris.predict_proba(iris_test_data);\n",
    "\n",
    "#Train the decision tree classifier on the Iris data\n",
    "dtree_iris = tree.DecisionTreeClassifier();\n",
    "dtree_iris.fit(iris_train_data, iris_train_labels);\n",
    "dtree_iris_predictions = dtree_iris.predict(iris_test_data);\n",
    "dtree_iris_predictions_proba = dtree_iris.predict_proba(iris_test_data);\n",
    "\n",
    "#The classification report is a handy tool to see many metrics from one command\n",
    "print \"Dummy classifier report\"\n",
    "print metrics.classification_report(iris_test_labels, dummy_iris_predictions)\n",
    "\n",
    "print \"Iris classifier report\"\n",
    "print metrics.classification_report(iris_test_labels, dtree_iris_predictions)\n",
    "\n",
    "#We can compare the two predictions using the t-test (from scipy)\n",
    "stats.ttest_rel(dummy_iris_predictions, dtree_iris_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Evaluation Metrics: Discrete Predictions (10 points)\n",
    "1.   Compute the accuracy for each classifier\n",
    "2.   Make a simple train-test split of gendata (using `randomstate=20160121`) and perform the same analysis: classification report and t-test\n",
    "3.   One of the issues in realistic evaluation settings is that the test outputs are not always generated in the same testing regime. Perform 5-fold cross-validation using the DummyClassifier and 10-fold cross-validation using the DecisionTree classifier. Read the documentation to determine the correct t-test function to use to compare these two sets of results, and perform the t-test across folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Question 5 Answers </h1>\n",
    "<p>1. Dummy Classifier accuracy: 0.421052631579 <br>\n",
    "Decision Tree Classifier accuracy: 0.894736842105</p>\n",
    "<p>2. Ttest_relResult(statistic=-0.74098388608089094, pvalue=0.45976285530296646)\n",
    "\n",
    "see cell below for better formattting </p>\n",
    "<p>3. Ttest_indResult(statistic=14.289053827191161, pvalue=2.509241547322248e-09)</p>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part 2\n",
    "Dummy classifier report\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.45      0.49      0.47        78\n",
    "          1       0.50      0.46      0.48        87\n",
    "\n",
    "avg / total       0.47      0.47      0.47       165\n",
    "\n",
    "Iris classifier report\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.86      0.86      0.86        78\n",
    "          1       0.87      0.87      0.87        87\n",
    "\n",
    "avg / total       0.87      0.87      0.87       165\n",
    "\n",
    "Ttest_relResult(statistic=-0.74098388608089094, pvalue=0.45976285530296646)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print \"Dummy Classifier accuracy: \" + str(accuracy_score(iris_test_labels, dummy_iris_predictions))\n",
    "print \"Decision Tree Classifier accuracy: \" + str(accuracy_score(iris_test_labels, dtree_iris_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2 \n",
    "[gendata_train_data_33, gendata_test_data_33,  gendata_train_labels_33, gendata_test_labels_33] = cross_validation.train_test_split(gendata_d, gendata_t, test_size=0.33, random_state=20160121)\n",
    "\n",
    "dummy_classifier_gendata = dummy.DummyClassifier();\n",
    "dummy_classifier_gendata.fit(gendata_train_data_33, gendata_train_labels_33)\n",
    "dummy_gendata_predictions = dummy_classifier_gendata.predict(gendata_test_data_33)\n",
    "dummy_gendata_predictions_proba = dummy_classifier_gendata.predict_proba(gendata_test_data_33)\n",
    "\n",
    "dtree_gendata = tree.DecisionTreeClassifier();\n",
    "dtree_gendata.fit(gendata_train_data_33, gendata_train_labels_33)\n",
    "dtree_gendata_predictions = dtree_gendata.predict(gendata_test_data_33)\n",
    "dtree_gendata_predictions_prob = dtree_gendata.predict_proba(gendata_test_data_33)\n",
    "\n",
    "#The classification report is a handy tool to see many metrics from one command\n",
    "print \"Dummy classifier report\"\n",
    "print metrics.classification_report(gendata_test_labels_33, dummy_gendata_predictions)\n",
    "\n",
    "print \"Iris classifier report\"\n",
    "print metrics.classification_report(gendata_test_labels_33, dtree_gendata_predictions)\n",
    "\n",
    "#We can compare the two predictions using the t-test (from scipy)\n",
    "print stats.ttest_rel(dummy_gendata_predictions, dtree_gendata_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 3\n",
    "from sklearn.metrics import accuracy_score\n",
    "kfolds = cross_validation.KFold(500, n_folds=5, shuffle=True, random_state=20160121)\n",
    "fold = 0\n",
    "\n",
    "dummy_accuracy = []\n",
    "for train, test in kfolds:\n",
    "    fold += 1\n",
    "    train_fold = gendata_d[train]\n",
    "    train_labels = gendata_t[train]\n",
    "    \n",
    "    test_fold = gendata_d[test]\n",
    "    test_labels = gendata_t[test]\n",
    "    \n",
    "    dummy_classifier_gendata = dummy.DummyClassifier();\n",
    "    dummy_classifier_gendata.fit(train_fold, train_labels)\n",
    "    dummy_gendata_predictions = dummy_classifier_gendata.predict(test_fold)\n",
    "    dummy_gendata_predictions_proba = dummy_classifier_gendata.predict_proba(test_fold)\n",
    "    \n",
    "    score = 1 - accuracy_score(test_labels, dummy_gendata_predictions)\n",
    "#     score = accuracy_score(test_labels, dummy_gendata_predictions)\n",
    "    dummy_accuracy.append(score)\n",
    "\n",
    "dtree_accuracy = []\n",
    "kfolds = cross_validation.KFold(500, n_folds=10, shuffle=True, random_state=20160121)\n",
    "for train, test in kfolds:\n",
    "    fold += 1\n",
    "    train_fold = gendata_d[train]\n",
    "    train_labels = gendata_t[train]\n",
    "    \n",
    "    test_fold = gendata_d[test]\n",
    "    test_labels = gendata_t[test]\n",
    "    \n",
    "    dtree_gendata = tree.DecisionTreeClassifier();\n",
    "    dtree_gendata.fit(train_fold, train_labels)\n",
    "    dtree_gendata_predictions = dtree_gendata.predict(test_fold)\n",
    "    dtree_gendata_predictions_prob = dtree_gendata.predict_proba(test_fold)\n",
    "    \n",
    "    score = 1 - accuracy_score(test_labels, dtree_gendata_predictions)\n",
    "#     score = accuracy_score(test_labels, dtree_gendata_predictions)\n",
    "    dtree_accuracy.append(score)\n",
    "    \n",
    "#We can compare the two predictions using the t-test (from scipy)\n",
    "print dummy_accuracy\n",
    "print dtree_accuracy\n",
    "stats.ttest_ind(dummy_accuracy, dtree_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
